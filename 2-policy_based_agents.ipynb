{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments which pose the full problem to an agent are referred to as Markov Decision Processes (MDPs). These environments not only provide rewards and state transitions given actions, but those rewards are also condition on the state of the environment and the action the agent takes within that state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/derrkater/anaconda/envs/gym_openai/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np\n",
    "import gym\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "\n",
    "def discount_rewards(rewards):\n",
    "    discounted_rewards = np.zeros_like(rewards)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, rewards.size)):\n",
    "        discounted_rewards[t] = running_add = running_add * gamma + rewards[t]\n",
    "        \n",
    "    return discounted_rewards\n",
    "\n",
    "class agent:\n",
    "    def __init__(self, learning_rate, states_size, actions_size, hidden_size):\n",
    "        # Forward\n",
    "        self.input_state = tf.placeholder(shape=[None, states_size], dtype=tf.float32)\n",
    "        hidden = slim.fully_connected(self.input_state, hidden_size, biases_initializer=None, activation_fn=tf.nn.relu)\n",
    "        self.output = slim.fully_connected(hidden, actions_size, activation_fn=tf.nn.softmax, biases_initializer=None)\n",
    "        self.chosen_action = tf.argmax(self.output, 1)\n",
    "        \n",
    "        # Backward\n",
    "        self.reward_holder = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        self.action_holder = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "        \n",
    "        self.indexes = tf.range(0, tf.shape(self.output)[0]) * tf.shape(self.output)[1] + self.action_holder\n",
    "        self.responsible_outputs = tf.gather(tf.reshape(self.output, [-1]), self.indexes)\n",
    "        self.loss = -tf.reduce_mean(tf.log(self.responsible_outputs) * self.reward_holder)\n",
    "        \n",
    "        trainable_variables = tf.trainable_variables()\n",
    "        self.gradient_holders = []\n",
    "        for idx, var in enumerate(trainable_variables):\n",
    "            placeholder = tf.placeholder(tf.float32, name='{}_holder'.format(idx))\n",
    "            self.gradient_holders.append(placeholder)\n",
    "            \n",
    "        self.gradients = tf.gradients(self.loss, trainable_variables)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        self.update_batch = optimizer.apply_gradients(zip(self.gradient_holders, trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/derrkater/anaconda/envs/gym_openai/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.0\n",
      "26.19\n",
      "36.21\n",
      "46.36\n",
      "50.5\n",
      "EPISODE 0\n",
      "EPISODE 1\n",
      "EPISODE 2\n",
      "EPISODE 3\n",
      "EPISODE 4\n",
      "EPISODE 5\n",
      "EPISODE 6\n",
      "EPISODE 7\n",
      "EPISODE 8\n",
      "EPISODE 9\n",
      "EPISODE 10\n",
      "EPISODE 11\n",
      "EPISODE 12\n",
      "EPISODE 13\n",
      "EPISODE 14\n",
      "EPISODE 15\n",
      "EPISODE 16\n",
      "EPISODE 17\n",
      "EPISODE 18\n",
      "EPISODE 19\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "my_agent = agent(learning_rate=1e-2, \n",
    "                 states_size=env.observation_space.shape[0], \n",
    "                 actions_size=env.action_space.n, \n",
    "                 hidden_size=8)\n",
    "\n",
    "total_episodes = 500\n",
    "max_ep = 999\n",
    "update_frequency = 5\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    total_reward = []\n",
    "    total_length = []\n",
    "    \n",
    "    gradient_buffer = sess.run(tf.trainable_variables())\n",
    "    for idx, gradient in enumerate(gradient_buffer):\n",
    "        gradient_buffer[idx] = gradient * 0\n",
    "    \n",
    "    for i in range(total_episodes):\n",
    "        state = env.reset()\n",
    "        running_reward = 0\n",
    "        ep_history = []\n",
    "        for j in range(max_ep):\n",
    "            actions_distribution = sess.run(my_agent.output, feed_dict={my_agent.input_state: [state]})  # [[p_1, p_2]]\n",
    "            # TODO: why not just random choice over indexes i.e. range()\n",
    "            action = np.random.choice(actions_distribution[0], p=actions_distribution[0])  # p\n",
    "            action = np.argmax(actions_distribution == action)  # index\n",
    "\n",
    "            state_1, reward, done, _ = env.step(action)\n",
    "            ep_history.append([state, action, reward, state_1])\n",
    "            state = state_1\n",
    "            running_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                ep_history = np.array(ep_history)\n",
    "                ep_history[:, 2] = discount_rewards(ep_history[:, 2])\n",
    "                feed_dict = {\n",
    "                    my_agent.reward_holder: ep_history[:, 2],\n",
    "                    my_agent.action_holder: ep_history[:, 1],\n",
    "                    my_agent.input_state: np.vstack(ep_history[:, 0])\n",
    "                }\n",
    "                \n",
    "                gradients = sess.run(my_agent.gradients, feed_dict=feed_dict)\n",
    "                for idx, gradient in enumerate(gradients):\n",
    "                    gradient_buffer[idx] += gradient\n",
    "                    \n",
    "                if i % update_frequency == 0 and i != 0:\n",
    "                    feed_dict = dictionary = dict(zip(my_agent.gradient_holders, gradient_buffer))\n",
    "                    _ = sess.run(my_agent.update_batch, feed_dict=feed_dict)\n",
    "                    for idx, gradient in enumerate(gradient_buffer):\n",
    "                        gradient_buffer[idx] = gradient * 0\n",
    "                        \n",
    "                total_reward.append(running_reward)\n",
    "                total_length.append(j)\n",
    "                break\n",
    "        \n",
    "        if not i % 100:\n",
    "            print(np.mean(total_reward[-100:]))\n",
    "            \n",
    "    for i_episode in range(20):\n",
    "        print('EPISODE {}'.format(i_episode))\n",
    "        observation = env.reset()\n",
    "        for t in range(100):\n",
    "            env.render()\n",
    "            actions_distribution = sess.run(my_agent.output, feed_dict={my_agent.input_state: [observation]})\n",
    "            action = np.random.choice(actions_distribution[0], p=actions_distribution[0])  # p\n",
    "            action = np.argmax(actions_distribution == action)  # index\n",
    "            observation, reward, done, info = env.step(env.action_space.sample())  # take a random action\n",
    "            if done:\n",
    "                break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'fully_connected/weights:0' shape=(4, 8) dtype=float32_ref>, <tf.Variable 'fully_connected_1/weights:0' shape=(8, 2) dtype=float32_ref>]\n",
      "[<tf.Variable 'fully_connected/weights:0' shape=(4, 8) dtype=float32_ref>, <tf.Variable 'fully_connected_1/weights:0' shape=(8, 2) dtype=float32_ref>, <tf.Variable 'beta1_power:0' shape=() dtype=float32_ref>, <tf.Variable 'beta2_power:0' shape=() dtype=float32_ref>, <tf.Variable 'fully_connected/weights/Adam:0' shape=(4, 8) dtype=float32_ref>, <tf.Variable 'fully_connected/weights/Adam_1:0' shape=(4, 8) dtype=float32_ref>, <tf.Variable 'fully_connected_1/weights/Adam:0' shape=(8, 2) dtype=float32_ref>, <tf.Variable 'fully_connected_1/weights/Adam_1:0' shape=(8, 2) dtype=float32_ref>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'fully_connected/weights:0' shape=(4, 8) dtype=float32_ref>,\n",
       " <tf.Variable 'fully_connected_1/weights:0' shape=(8, 2) dtype=float32_ref>,\n",
       " <tf.Variable 'beta1_power:0' shape=() dtype=float32_ref>,\n",
       " <tf.Variable 'beta2_power:0' shape=() dtype=float32_ref>,\n",
       " <tf.Variable 'fully_connected/weights/Adam:0' shape=(4, 8) dtype=float32_ref>,\n",
       " <tf.Variable 'fully_connected/weights/Adam_1:0' shape=(4, 8) dtype=float32_ref>,\n",
       " <tf.Variable 'fully_connected_1/weights/Adam:0' shape=(8, 2) dtype=float32_ref>,\n",
       " <tf.Variable 'fully_connected_1/weights/Adam_1:0' shape=(8, 2) dtype=float32_ref>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tf.trainable_variables())\n",
    "print(tf.global_variables())\n",
    "tf.global_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('__variable_store',),\n",
       " 'update_ops',\n",
       " 'train_op',\n",
       " ('__varscope',),\n",
       " 'trainable_variables',\n",
       " 'model_variables',\n",
       " 'variables']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = tf.get_default_graph()\n",
    "graph.collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'fully_connected/weights:0' shape=(4, 8) dtype=float32_ref>,\n",
       " <tf.Variable 'fully_connected_1/weights:0' shape=(8, 2) dtype=float32_ref>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_collection('model_variables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "for i_episode in range(20):\n",
    "#     print('EPISODE {}'.format(i_episode))\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        Image.fromarray(env.render(mode='rgb_array')).resize((320, 420))\n",
    "#         env.render()\n",
    "        observation, reward, done, info = env.step(env.action_space.sample())  # take a random action\n",
    "        if done:\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
